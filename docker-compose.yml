version: "3.9"

# ============================================================================
# PLOS - Personal Life Management System
# Backend Services - Production Ready (Clean Version)
# ============================================================================

services:
  # ==========================================================================
  # INFRASTRUCTURE LAYER - SUPABASE
  # ==========================================================================

  # Supabase PostgreSQL Database
  supabase-db:
    image: supabase/postgres:15.8.1.060
    container_name: plos-supabase-db
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-plos_db_secure_2025}
      POSTGRES_DB: ${POSTGRES_DB:-plos}
      JWT_SECRET: ${JWT_SECRET:-your-super-secret-jwt-token-with-at-least-32-characters-long}
      JWT_EXP: ${JWT_EXP:-3600}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - supabase_db_data:/var/lib/postgresql/data
      - ./infrastructure/database/00-supabase-admin.sh:/docker-entrypoint-initdb.d/00-supabase-admin.sh
      - ./infrastructure/database/init.sql:/docker-entrypoint-initdb.d/zz-01-app-init.sql
      - ./infrastructure/database/seed.sql:/docker-entrypoint-initdb.d/zz-02-seed.sql
      - ./infrastructure/database/migrations:/docker-entrypoint-initdb.d/app-migrations
      - ./infrastructure/database/zz-03-run-app-migrations.sh:/docker-entrypoint-initdb.d/zz-03-run-app-migrations.sh
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: 1.2G
        reservations:
          cpus: "0.5"
          memory: 512M
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-plos}",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Supabase Studio - Database Management UI
  supabase-studio:
    image: supabase/studio:20241202-71e5240
    container_name: plos-supabase-studio
    profiles: ["studio"]
    depends_on:
      supabase-db:
        condition: service_healthy
    environment:
      STUDIO_PG_META_URL: http://supabase-meta:8080
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-plos_db_secure_2025}
      DEFAULT_ORGANIZATION_NAME: PLOS
      DEFAULT_PROJECT_NAME: "Personal Life OS"
      SUPABASE_URL: http://localhost:8000
      SUPABASE_PUBLIC_URL: http://localhost:8000
      SUPABASE_ANON_KEY: ${SUPABASE_ANON_KEY:-eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6ImFub24iLCJleHAiOjE5ODM4MTI5OTZ9.CRXP1A7WOeoJeXxjNni43kdQwgnWNReilDMblYTn_I0}
      SUPABASE_SERVICE_KEY: ${SUPABASE_SERVICE_KEY:-eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZS1kZW1vIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImV4cCI6MTk4MzgxMjk5Nn0.EGIM96RAZx35lJzdJsyH-qQwv8Hdp7fsn3W0YpN81IU}
      NEXT_PUBLIC_ENABLE_LOGS: "true"
      NEXT_ANALYTICS_BACKEND_PROVIDER: postgres
    ports:
      - "${SUPABASE_STUDIO_PORT:-3000}:3000"
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/profile"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Supabase Meta - Required for Studio
  supabase-meta:
    image: supabase/postgres-meta:v0.84.2
    container_name: plos-supabase-meta
    profiles: ["studio"]
    depends_on:
      supabase-db:
        condition: service_healthy
    environment:
      PG_META_PORT: 8080
      PG_META_DB_HOST: supabase-db
      PG_META_DB_PORT: 5432
      PG_META_DB_NAME: ${POSTGRES_DB:-plos}
      PG_META_DB_USER: ${POSTGRES_USER:-postgres}
      PG_META_DB_PASSWORD: ${POSTGRES_PASSWORD:-plos_db_secure_2025}
    ports:
      - "${META_PORT:-8085}:8080"
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 64M
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Redis Cache & Session Store
  redis:
    image: redis:7-alpine
    container_name: plos-redis
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD:-plos_redis_secure_2025}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis_data:/data
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.1"
          memory: 64M
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # Zookeeper (Required for Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: plos-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_datalog:/var/lib/zookeeper/log
    networks:
      - plos-network
    restart: unless-stopped

  # Apache Kafka (Event Streaming)
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: plos-kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9092:9092"
      - "29092:29092"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "0.8"
          memory: 768M
        reservations:
          cpus: "0.25"
          memory: 256M
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"
    healthcheck:
      test:
        [
          "CMD",
          "kafka-broker-api-versions",
          "--bootstrap-server",
          "localhost:9092",
        ]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Kafka UI (Optional - for monitoring)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: plos-kafka-ui
    profiles: ["ui"]
    depends_on:
      - kafka
    environment:
      KAFKA_CLUSTERS_0_NAME: plos-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    ports:
      - "${KAFKA_UI_PORT:-8080}:8080"
    networks:
      - plos-network
    restart: unless-stopped

  # ==========================================================================
  # API GATEWAY
  # ==========================================================================

  api-gateway:
    build:
      context: .
      dockerfile: ./services/api-gateway/Dockerfile
    container_name: plos-api-gateway
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /etc/kong/kong.yml
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: "0.0.0.0:8001"
    ports:
      - "${API_GATEWAY_PORT:-8000}:8000"
      - "8501:8001"
    networks:
      - plos-network
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ==========================================================================
  # CORE SERVICES (AI-Powered)
  # ==========================================================================

  # Context Broker - Central state management
  context-broker:
    build:
      context: .
      dockerfile: ./services/context-broker/Dockerfile
    container_name: plos-context-broker
    depends_on:
      supabase-db:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
    environment:
      SERVICE_NAME: context-broker
      SERVICE_PORT: 8001
      POSTGRES_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-plos_db_secure_2025}@supabase-db:5432/${POSTGRES_DB:-plos}
      REDIS_URL: redis://:${REDIS_PASSWORD:-plos_redis_secure_2025}@redis:6379/0
      KAFKA_BROKERS: kafka:9092
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      GEMINI_API_KEY: ${GEMINI_API_KEY:-}
      GEMINI_API_KEYS: ${GEMINI_API_KEYS:-}
      GEMINI_DEFAULT_MODEL: ${GEMINI_DEFAULT_MODEL:-gemini-2.5-flash}
    ports:
      - "${CONTEXT_BROKER_PORT:-8001}:8001"
    volumes:
      - ./shared:/app/shared:ro
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M
    security_opt:
      - no-new-privileges:true
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Journal Parser - AI-powered extraction with Gemini
  journal-parser:
    build:
      context: .
      dockerfile: ./services/journal-parser/Dockerfile
    container_name: plos-journal-parser
    depends_on:
      supabase-db:
        condition: service_healthy
      kafka:
        condition: service_healthy
      context-broker:
        condition: service_healthy
    environment:
      SERVICE_NAME: journal-parser
      SERVICE_PORT: 8002
      POSTGRES_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-plos_db_secure_2025}@supabase-db:5432/${POSTGRES_DB:-plos}
      KAFKA_BROKERS: kafka:9092
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      GEMINI_API_KEY: ${GEMINI_API_KEY:-}
      GEMINI_API_KEYS: ${GEMINI_API_KEYS:-}
      GEMINI_DEFAULT_MODEL: ${GEMINI_DEFAULT_MODEL:-gemini-2.5-flash}
      GEMINI_API_KEY_ROTATION_ENABLED: ${GEMINI_API_KEY_ROTATION_ENABLED:-true}
      GEMINI_API_KEY_ROTATION_MAX_RETRIES: ${GEMINI_API_KEY_ROTATION_MAX_RETRIES:-3}
      GEMINI_API_KEY_ROTATION_BACKOFF_SECONDS: ${GEMINI_API_KEY_ROTATION_BACKOFF_SECONDS:-60}
      USE_GEMINI_CACHING: ${USE_GEMINI_CACHING:-true}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    ports:
      - "${JOURNAL_PARSER_PORT:-8002}:8002"
    volumes:
      - ./shared:/app/shared:ro
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 256M
    security_opt:
      - no-new-privileges:true
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/journal/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Knowledge System - Vector DB powered semantic search
  knowledge-system:
    build:
      context: .
      dockerfile: ./services/knowledge-system/Dockerfile
    container_name: plos-knowledge-system
    depends_on:
      supabase-db:
        condition: service_healthy
      kafka:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    environment:
      SERVICE_NAME: knowledge-system
      SERVICE_PORT: 8003
      POSTGRES_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-plos_db_secure_2025}@supabase-db:5432/${POSTGRES_DB:-plos}
      KAFKA_BROKERS: kafka:9092
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      QDRANT_URL: http://qdrant:6333
      QDRANT_API_KEY: ${QDRANT_API_KEY:-qdrant_secure_key_2025}
      GEMINI_API_KEY: ${GEMINI_API_KEY:-}
      GEMINI_API_KEYS: ${GEMINI_API_KEYS:-}
      GEMINI_DEFAULT_MODEL: ${GEMINI_DEFAULT_MODEL:-gemini-2.5-flash}
      GEMINI_EMBEDDING_MODEL: ${GEMINI_EMBEDDING_MODEL:-text-embedding-004}
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-minio:9000}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-plos_minio_admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-plos_minio_secure_2025}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    ports:
      - "${KNOWLEDGE_SYSTEM_PORT:-8003}:8003"
    volumes:
      - ./shared:/app/shared:ro
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 1.5G
        reservations:
          cpus: "1.0"
          memory: 512M
    security_opt:
      - no-new-privileges:true
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # Qdrant Vector Database - Semantic search engine
  qdrant:
    image: qdrant/qdrant:latest
    container_name: plos-qdrant
    environment:
      QDRANT__SERVICE__API_KEY: ${QDRANT_API_KEY:-qdrant_secure_key_2025}
      QDRANT__SERVICE__ENABLE_TLS: "false"
    ports:
      - "${QDRANT_REST_PORT:-6333}:6333" # REST API
      - "${QDRANT_GRPC_PORT:-6334}:6334" # gRPC API
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"
    healthcheck:
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/6333' || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # Meilisearch - Typo-tolerant Full-Text Search
  meilisearch:
    image: getmeili/meilisearch:v1.6
    container_name: plos-meilisearch
    environment:
      MEILI_MASTER_KEY: ${MEILI_MASTER_KEY:-meilisearch_secure_key_2025}
      MEILI_ENV: development
      MEILI_NO_ANALYTICS: "true"
    ports:
      - "${MEILISEARCH_PORT:-7700}:7700"
    volumes:
      - meilisearch_data:/meili_data
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7700/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # MinIO - Self-hosted Object Storage (S3-compatible)
  minio:
    image: minio/minio:latest
    container_name: plos-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-plos_minio_admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-plos_minio_secure_2025}
    ports:
      - "${MINIO_API_PORT:-9000}:9000" # API
      - "${MINIO_CONSOLE_PORT:-9001}:9001" # Console
    volumes:
      - minio_data:/data
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 256M
        reservations:
          cpus: "0.25"
          memory: 128M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ==========================================================================
  # MONITORING & OBSERVABILITY
  # ==========================================================================

  # NOTE: pgAdmin removed - Use Supabase Studio at http://localhost:3000 instead

  # Redis Commander - Redis Web UI
  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: plos-redis-commander
    profiles: ["ui"]
    depends_on:
      - redis
    environment:
      REDIS_HOSTS: local:redis:6379:0:${REDIS_PASSWORD:-plos_redis_secure_2025}
    ports:
      - "${REDIS_COMMANDER_PORT:-8081}:8081"
    networks:
      - plos-network
    restart: unless-stopped

  # Prometheus - Metrics collection
  prometheus:
    image: prom/prometheus:latest
    container_name: plos-prometheus
    profiles: ["monitoring"]
    volumes:
      - ./infrastructure/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
    networks:
      - plos-network
    restart: unless-stopped

  # Grafana - Dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: plos-grafana
    profiles: ["monitoring"]
    depends_on:
      - prometheus
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"
    ports:
      - "${GRAFANA_PORT:-3333}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - plos-network
    restart: unless-stopped

  # Metabase - Business Intelligence & Data Visualization
  metabase:
    image: metabase/metabase:latest
    container_name: plos-metabase
    profiles: ["bi"]
    depends_on:
      supabase-db:
        condition: service_healthy
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: metabase
      MB_DB_PORT: 5432
      MB_DB_USER: ${POSTGRES_USER:-postgres}
      MB_DB_PASS: ${POSTGRES_PASSWORD:-plos_db_secure_2025}
      MB_DB_HOST: supabase-db
      JAVA_OPTS: -Xmx1g
    ports:
      - "${METABASE_PORT:-3001}:3000"
    volumes:
      - metabase_data:/metabase-data
    networks:
      - plos-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.25"
          memory: 256M
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

# ============================================================================
# NETWORKING
# ============================================================================

networks:
  plos-network:
    driver: bridge
    name: plos-network

# ============================================================================
# VOLUMES
# ============================================================================

volumes:
  supabase_db_data:
    driver: local
  redis_data:
    driver: local
  kafka_data:
    driver: local
  zookeeper_data:
    driver: local
  zookeeper_datalog:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  qdrant_data:
    driver: local
  metabase_data:
    driver: local
  meilisearch_data:
    driver: local
  minio_data:
    driver: local
