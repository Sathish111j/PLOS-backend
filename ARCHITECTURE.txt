================================================================================
                    PLOS ARCHITECTURE OVERVIEW
        Personal Life Management System - Complete End-to-End Design
================================================================================

Last Updated: December 29, 2025

================================================================================
TABLE OF CONTENTS
================================================================================

1. System Architecture
2. Service Architecture (Detailed)
3. Data Flow Architecture (End-to-End Examples)
4. Infrastructure Layer
5. Kafka Topic Architecture
6. Data Models & Relationships
7. AI Integration (Gemini)
8. Request Flow Examples
9. Caching Strategy
10. Monitoring & Observability
11. Design Patterns
12. Deployment & Runtime
13. Key Design Principles

================================================================================
1. SYSTEM ARCHITECTURE
================================================================================

VISUAL OVERVIEW:

┌─────────────────────────────────────────────────────────────────┐
│                         CLIENT LAYER                             │
│                    (Frontend Application)                        │
└────────────────────────────┬────────────────────────────────────┘
                             │
                     HTTP/REST Requests
                             │
         ┌───────────────────▼───────────────────┐
         │     API GATEWAY (Kong)                │
         │  Port: 8000                           │
         │  - Route management                   │
         │  - Request validation                 │
         │  - Load balancing                     │
         │  - Authentication/Authorization       │
         └───────────────────┬───────────────────┘
                             │
        ┌────────────────────┼────────────────────┐
        │                    │                    │
   ┌────▼────────┐    ┌──────▼────────┐   ┌──────▼────────┐
   │   CONTEXT    │    │   JOURNAL      │   │  KNOWLEDGE    │
   │   BROKER     │    │   PARSER       │   │  SYSTEM       │
   │   Port:8001  │    │   Port:8002    │   │  Port:8003    │
   └────┬────────┘    └──────┬────────┘   └──────┬────────┘
        │                    │                    │
        └────────────────────┼────────────────────┘
                             │
                  Async Event Publishing/Subscribing
                             │
        ┌────────────────────▼────────────────────┐
        │   APACHE KAFKA (Event Streaming)        │
        │   Port: 9092                            │
        │   - Topic-based message queue           │
        │   - Event broadcasting                  │
        │   - Persistent event log                │
        └────────────────────┬────────────────────┘
                             │
         ┌───────────────────┼───────────────────┐
         │                   │                   │
   ┌─────▼─────┐    ┌────────▼────────┐  ┌──────▼──────┐
   │POSTGRESQL  │    │   REDIS CACHE   │  │   QDRANT    │
   │Port:5432   │    │  Port:6379      │  │ Vector DB   │
   │- Relational│    │- Hot cache      │  │ Port:6333   │
   │- TimescaleDB    │- Sessions       │  │- Semantic   │
   │- Event Logs│    │- Context cache  │  │ search      │
   └────────────┘    └─────────────────┘  └─────────────┘


KEY COMPONENTS:

- Client: Frontend web/mobile application
- API Gateway: Kong - single entry point for all requests
- Microservices: Independent services handling specific domains
- Kafka: Event bus for async communication
- PostgreSQL: OLTP database + TimescaleDB for time-series
- Redis: Hot cache for frequently accessed data
- Qdrant: Vector database for semantic search
- Monitoring: Prometheus + Grafana

================================================================================
2. SERVICE ARCHITECTURE (DETAILED)
================================================================================

2.1 API GATEWAY (Kong)
─────────────────────

Purpose: Single entry point for all client requests
Port: 8000
Database: PostgreSQL (Kong configuration)

Responsibilities:
  - Route requests to appropriate microservices
  - JWT authentication & authorization
  - Rate limiting (100 req/min anonymous, 1000 req/min authenticated)
  - Request/response transformation
  - Logging and monitoring
  - Load balancing across service instances

Access Points:
  - API Gateway: http://localhost:8000
  - Admin API: http://localhost:8001 (Kong admin)


2.2 CONTEXT BROKER SERVICE
──────────────────────────

Purpose: Central source of truth for user state
Port: 8001
Language: Python (FastAPI)

Key Components:

  a) CacheManager
     - Manages Redis connections
     - Cache hit/miss logic
     - TTL management (5-30 minutes)
     - Async operations

  b) StateManager
     - PostgreSQL connection management
     - State persistence
     - Query aggregation
     - Transaction handling

  c) ContextEngine
     - Core business logic
     - Cache-aside pattern implementation
     - Data aggregation from multiple sources
     - Cache invalidation

Responsibilities:
  - Maintains real-time user context (single source of truth)
  - Aggregates data from multiple sources
  - Cache-first retrieval strategy for sub-50ms latency
  - Invalidates cache on updates
  - Computes rolling averages (7-day)
  - Tracks task/goal counts

Consumes:
  - CONTEXT_UPDATES (Kafka topic)

Publishes:
  - None (read-only from Kafka perspective)

API Endpoints:
  - GET /api/context/{user_id} - Get complete user context
  - GET /api/context/{user_id}/summary - Lightweight summary
  - POST /api/context/update - Manual context update
  - GET /health - Health check


2.3 JOURNAL PARSER SERVICE
──────────────────────────

Purpose: AI-powered journal entry extraction using Gemini
Port: 8002
Language: Python (FastAPI)

Key Components:

  a) GeminiExtractor (generalized_extraction.py)
     - Integrates with Google Gemini API
     - Structured output schema using Pydantic
     - Comprehensive multi-field extraction
     - Controlled vocabulary normalization
     - Confidence scoring per field

  b) GapResolver (generalized_extraction.py)
     - Analyzes extracted data for missing fields
     - Generates clarification questions for user
     - Identifies ambiguous entries
     - Priority-based gap handling

  c) KafkaJournalConsumer
     - Async event listener
     - Message deserialization
     - Error handling and retry logic
     - Offset management

Responsibilities:
  - Consumes raw journal entries from Kafka
  - Calls Gemini API to extract structured data
  - Extracts: mood, health, nutrition, exercise, work metrics, habits
  - Detects missing information
  - Computes confidence scores
  - Publishes parsed results and individual events
  - Validates and transforms data

Consumes:
  - JOURNAL_ENTRIES (raw user journal text)

Publishes:
  - PARSED_ENTRIES (complete parsed structure)
  - MOOD_EVENTS (mood scores and labels)
  - HEALTH_EVENTS (sleep, energy, stress)
  - NUTRITION_EVENTS (meals, calories)
  - EXERCISE_EVENTS (workouts, duration)
  - WORK_EVENTS (productivity metrics)
  - CONTEXT_UPDATES (for Context Broker)

Gemini Models Used:
  - gemini-2.5-flash (default - balanced performance)
  - gemini-2.0-flash-exp (alternative)
  - Caching: Enabled (reduces API costs ~25%)

Example Extraction Output:
  {
    "mood_score": 8.0,
    "mood_labels": ["energized", "focused", "productive"],
    "sleep_hours": 8.0,
    "sleep_quality": 9,
    "energy_level": 8,
    "stress_level": 3,
    "exercise_minutes": 30,
    "exercise_type": "running",
    "exercise_intensity": "moderate",
    "work_hours": 6.0,
    "productivity_score": 8.0,
    "focus_level": 8,
    "meals_count": 3,
    "water_intake": 2.5,
    "confidence_score": 0.95,
    "gaps_detected": [],
    "parsed_at": "2025-01-15T10:30:00Z"
  }


2.4 KNOWLEDGE SYSTEM SERVICE
─────────────────────────────

Purpose: Semantic search and knowledge management
Port: 8003
Language: Python (FastAPI)

Key Components:

  a) VectorStore
     - Qdrant vector database integration
     - Embedding generation via Gemini
     - Vector similarity search
     - Collection management
     - API key authentication

  b) KnowledgeExtractor
     - Gemini-powered content extraction
     - Topic identification
     - Tag generation
     - Semantic enrichment
     - Relationship detection

Responsibilities:
  - Stores knowledge items with semantic embeddings
  - Provides similarity-based search
  - Manages personal wiki/knowledge base
  - Extracts topics and tags
  - Handles file uploads (documents, notes)
  - Semantic relationship discovery

Consumes:
  - KNOWLEDGE_EVENTS (knowledge creation/update)

Publishes:
  - Updates to vector database

Gemini Models Used:
  - gemini-embedding-001 (vector embeddings)
  - gemini-2.0-flash-exp (knowledge extraction)

API Endpoints:
  - POST /api/knowledge - Create knowledge item
  - GET /api/knowledge/search - Semantic search
  - GET /api/knowledge/{id} - Get item details
  - DELETE /api/knowledge/{id} - Delete item


================================================================================
3. DATA FLOW ARCHITECTURE (END-TO-END EXAMPLES)
================================================================================

3.1 COMPLETE FLOW: JOURNAL ENTRY PROCESSING
─────────────────────────────────────────────

Step 1: USER WRITES JOURNAL
  User writes entry in frontend app
  
Step 2: ENTRY SUBMITTED TO API
  Frontend: POST /api/journal
  Body: {
    "user_id": "123e4567-e89b-12d3-a456-426614174000",
    "entry_date": "2025-01-15",
    "raw_text": "Woke up feeling great! 8hrs sleep. 5km run. Working on new project..."
  }

Step 3: API GATEWAY ROUTES REQUEST
  Kong validates JWT token
  Routes to Journal/Context service
  
Step 4: ENTRY STORED IN DATABASE
  PostgreSQL: INSERT INTO journal_entries
  Returns entry ID and timestamps
  
Step 5: KAFKA EVENT PUBLISHED
  Service publishes JOURNAL_ENTRIES topic:
  {
    "journal_entry_id": "uuid",
    "user_id": "uuid",
    "entry_date": "2025-01-15",
    "raw_text": "..."
  }

Step 6: JOURNAL PARSER CONSUMES
  KafkaJournalConsumer receives message
  Verifies message integrity
  Passes to JournalParserOrchestrator

Step 7: GEMINI AI EXTRACTION (Synchronous)
  GeminiExtractor calls Gemini API with structured schema
  Input: raw_text + user_context
  Output: Structured JSON with confidence score
  Takes ~2-3 seconds per entry
  Result cached in Redis

Step 8: GAP DETECTION
  GapResolver analyzes extracted fields
  Identifies ambiguous or missing information
  Generates clarification questions:
    gaps_detected: ["activity_type", "meal_details"]
    questions: ["What sport did you play?", "What did you eat for lunch?"]

Step 9: PARSED ENTRY PUBLISHED
  Journal Parser publishes PARSED_ENTRIES:
  {
    "journal_entry_id": "uuid",
    "mood_score": 8,
    "health": {...},
    "nutrition": {...},
    "exercise": {...},
    "confidence_score": 0.95,
    "gaps_detected": [...]
  }

Step 10: INDIVIDUAL EVENTS PUBLISHED
  Journal Parser breaks down data into specific events:
  
  a) MOOD_EVENTS:
     {"user_id": "uuid", "mood_score": 8, ...}
     
  b) HEALTH_EVENTS:
     {"user_id": "uuid", "sleep_hours": 8, "energy_level": 8, ...}
     
  c) EXERCISE_EVENTS:
     {"user_id": "uuid", "exercise_type": "running", "duration": 30, ...}
     
  d) CONTEXT_UPDATES (aggregated):
     {"user_id": "uuid", "update_type": "mood", "data": {...}}

Step 11: CONTEXT BROKER SUBSCRIBES
  ContextBroker Kafka consumer listens to CONTEXT_UPDATES
  Receives mood, health, exercise, work, nutrition updates

Step 12: DATABASE UPDATE (State Manager)
  For each CONTEXT_UPDATE event:
  - UPDATE user_context SET current_mood_score = 8, ...
  - COMPUTE 7-day rolling averages:
    SELECT AVG(mood_score) OVER (
      ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
    )
  - Store metadata in JSON columns

Step 13: CACHE INVALIDATION
  ContextEngine invalidates Redis cache:
  Redis DELETE context:{user_id}
  Next request will fetch fresh data

Step 14: CLIENT FETCHES UPDATED CONTEXT
  Frontend: GET /api/context/{user_id}
  Context Broker:
    a) Check Redis cache
       IF hit THEN return immediately (< 10ms)
       ELSE continue
    
    b) Query PostgreSQL:
       SELECT current_mood_score, energy_level, ...
       SELECT AVG(mood_score) OVER (7 days)
       SELECT COUNT(*) FROM tasks WHERE ...
    
    c) Aggregate all data into UserContext object
    
    d) Cache in Redis (TTL: 5 minutes)
    
    e) Return JSON response

Step 15: FRONTEND DISPLAYS UPDATED STATE
  User sees updated mood, energy, stress levels
  See 7-day trends
  See pending tasks
  View any detected gaps

Total Latency: ~3-5 seconds (due to AI extraction)
Subsequent requests: <50ms (via cache)


3.2 FLOW: RETRIEVE USER CONTEXT
─────────────────────────────────

Frontend Request:
  GET /api/context/123e4567-e89b-12d3-a456-426614174000

API Gateway:
  1. Validate JWT token
  2. Extract user_id from claims
  3. Route to context-broker service

Context Broker Service:
  1. Receive request
  
  2. Try Cache (Redis):
     key = "context:123e4567..."
     IF key exists:
       value = redis.get(key)
       return UserContext(value)
       Latency: 5-10ms
     ELSE:
       Continue to step 3
  
  3. Query Database (PostgreSQL):
     Query 1: SELECT * FROM user_context WHERE user_id = ?
     Query 2: SELECT AVG(mood_score) 
              FROM mood_events 
              WHERE user_id = ? 
              AND created_at >= NOW() - 7 days
     Query 3: SELECT AVG(productivity_score) 
              FROM work_events 
              WHERE user_id = ? 
              AND created_at >= NOW() - 7 days
     Query 4: SELECT COUNT(*) FROM goals WHERE user_id = ?
     Query 5: SELECT COUNT(*) FROM tasks WHERE user_id = ? AND status != 'completed'
     Total Latency: 30-50ms
  
  4. Aggregate Results:
     Combine all query results
     Create UserContext object
     Add computed fields
     Latency: <1ms
  
  5. Cache in Redis:
     redis.set(key, json.dumps(context), ex=300)
     Latency: 5ms
  
  6. Return Response:
     HTTP 200 OK
     Body: UserContext JSON
     Total Latency: 40-60ms (first request), <10ms (cached)

Client Receives:
  {
    "user_id": "123e4567...",
    "current_mood_score": 8,
    "current_energy_level": 6,
    "current_stress_level": 4,
    "sleep_quality_avg_7d": 7.2,
    "productivity_score_avg_7d": 6.8,
    "active_goals_count": 5,
    "pending_tasks_count": 12,
    "completed_tasks_today": 3,
    "context_data": {...},
    "updated_at": "2025-01-15T10:30:00Z"
  }


================================================================================
4. INFRASTRUCTURE LAYER
================================================================================

4.1 DATA STORAGE SYSTEMS
────────────────────────

PostgreSQL 15 (Alpine)
  Port: 5432
  Container: plos-postgres
  Database: plos
  User: postgres
  Schema:
    - user_context (current state)
    - journal_entries (raw entries)
    - parsed_entries (AI extraction results)
    - mood_events (time-series)
    - health_events (time-series)
    - nutrition_events (time-series)
    - exercise_events (time-series)
    - work_events (time-series)
    - goals (user goals)
    - tasks (user tasks)
    - knowledge_items (personal wiki)
  
  Extensions: TimescaleDB (time-series optimization)
  Persistence: Docker volume `postgres_data`
  Health Check: pg_isready


Redis 7 (Alpine)
  Port: 6379
  Container: plos-redis
  Password: plos_redis_secure_2025 (configurable)
  Max Memory: 512MB
  Eviction Policy: allkeys-lru
  Persistence: AOF enabled
  
  Usage:
    - Session storage
    - Context cache (context:{user_id})
    - Mood cache (mood:{user_id}:{date})
    - Rate limiting counters
    - Real-time notifications
  
  Docker Volume: `redis_data`
  Health Check: INCR ping command


Qdrant Vector Database
  Port: 6333 (REST), 6334 (gRPC)
  Container: plos-qdrant
  API Key: qdrant_secure_key_2025 (configurable)
  
  Purpose: Semantic search via vector embeddings
  
  Collections:
    - knowledge_base (personal wiki items)
    - journal_summaries (journal entry embeddings)
    - habit_patterns (habit embeddings)
  
  Vector Dimension: 768 (from Gemini embeddings)
  Similarity Metric: Cosine
  Docker Volume: `qdrant_data`
  Health Check: TCP port 6333


4.2 MESSAGE BUS
───────────────

Apache Kafka 7.5.0 + Zookeeper 7.5.0
  Kafka Port: 9092 (broker), 29092 (host)
  Zookeeper Port: 2181
  
  Features:
    - Topic-based publish-subscribe
    - Persistent event log
    - Consumer groups for parallelization
    - Auto topic creation enabled
    - Offset management
  
  Containers:
    - plos-zookeeper (coordination)
    - plos-kafka (broker)
    - plos-kafka-ui (web UI)
  
  Kafka UI: http://localhost:8080
  
  Docker Volumes:
    - kafka_data
    - zookeeper_data
    - zookeeper_datalog


4.3 MONITORING & OBSERVABILITY
──────────────────────────────

Prometheus
  Port: 9090
  Container: plos-prometheus
  Config: ./infrastructure/monitoring/prometheus.yml
  
  Scrapes:
    - All service /metrics endpoints
    - Kafka JMX metrics
    - PostgreSQL exporter
    - Redis exporter
  
  Retention: 15 days (default)
  Docker Volume: `prometheus_data`


Grafana
  Port: 3333
  Container: plos-grafana
  Default Admin: admin/admin
  
  Dashboards:
    - System Health (uptime, latency, errors)
    - Business Metrics (entries processed, accuracy)
    - Infrastructure (storage, CPU, memory)
    - Kafka Consumer Lag
    - Database Performance
  
  Docker Volume: `grafana_data`


================================================================================
5. KAFKA TOPIC ARCHITECTURE
================================================================================

5.1 TOPIC DEFINITIONS
──────────────────────

INPUT TOPICS (User data entry):
  - journal_entries
    Subscribers: journal-parser
    Format: JournalEntry
    
  - task_events
    Subscribers: context-broker, scheduling-agents
    Format: TaskEvent
    
  - goal_events
    Subscribers: context-broker
    Format: GoalEvent
    
  - calendar_events
    Subscribers: context-broker, scheduling-agents
    Format: CalendarEvent


PARSED DATA TOPICS (AI extraction results):
  - parsed_entries
    Publishers: journal-parser
    Subscribers: knowledge-system, analytics
    Format: ParsedJournalEntry
    
  - mood_events
    Publishers: journal-parser
    Subscribers: context-broker
    Format: MoodEntry
    
  - health_events
    Publishers: journal-parser
    Subscribers: context-broker
    Format: HealthMetrics
    
  - nutrition_events
    Publishers: journal-parser
    Subscribers: context-broker
    Format: NutritionEntry
    
  - exercise_events
    Publishers: journal-parser
    Subscribers: context-broker
    Format: ExerciseEntry
    
  - work_events
    Publishers: journal-parser
    Subscribers: context-broker
    Format: WorkEvent
    
  - habit_events
    Publishers: journal-parser
    Subscribers: context-broker


AGGREGATION TOPICS (Aggregated state):
  - context_updates
    Publishers: Various services
    Subscribers: context-broker
    Format: ContextUpdate
    Description: Real-time state updates
    
  - knowledge_events
    Publishers: API endpoints, journal-parser
    Subscribers: knowledge-system
    Format: KnowledgeEvent
    Description: Knowledge base updates


AI REQUEST TOPICS (Agent requests):
  - insight_requests
    Publishers: Frontend, context-broker
    Subscribers: insight-agent (future)
    Format: InsightRequest
    Description: AI-generated insights
    
  - scheduling_requests
    Publishers: Task creation endpoints
    Subscribers: scheduling-agent (future)
    Format: SchedulingRequest
    Description: Smart scheduling


OUTPUT TOPICS (Notifications):
  - notification_events
    Publishers: All services
    Subscribers: notification-service (future)
    Format: NotificationEvent
    Description: User alerts, reminders


GENERAL EVENT STREAM:
  - event_stream
    Publishers: All services
    Subscribers: Analytics, archival
    Format: GenericEvent
    Description: Complete event log


5.2 CONSUMER GROUPS
────────────────────

plos-context-broker:
  Topics: context_updates
  Parallelism: 3 (3 partitions)
  Lag Monitoring: Via Prometheus
  
plos-journal-parser:
  Topics: journal_entries
  Parallelism: 3
  
plos-knowledge-system:
  Topics: knowledge_events
  Parallelism: 1


5.3 TOPIC CONFIGURATION
───────────────────────

All topics created with:
  Partitions: 3 (parallelization)
  Replication Factor: 1
  Retention: 7 days
  Cleanup Policy: delete
  Compression: snappy


================================================================================
6. DATA MODELS & RELATIONSHIPS
================================================================================

6.1 USER CONTEXT (Single Source of Truth)
──────────────────────────────────────────

UserContext:
  Fields:
    - user_id: UUID (primary key)
    - current_mood_score: int (1-10, nullable)
    - current_energy_level: int (1-10, nullable)
    - current_stress_level: int (1-10, nullable)
    - sleep_quality_avg_7d: float (7-day rolling average)
    - productivity_score_avg_7d: float (7-day rolling average)
    - active_goals_count: int
    - pending_tasks_count: int
    - completed_tasks_today: int
    - context_data: JSON (extensible)
    - updated_at: timestamp

  Database:
    Table: user_context
    Index: PRIMARY KEY (user_id)
    Index: created_at DESC (for recent lookups)
    Cache Key: context:{user_id}
    Cache TTL: 5 minutes


6.2 JOURNAL ENTRY PROCESSING PIPELINE
───────────────────────────────────────

JournalEntry (Raw Input):
  - id: UUID
  - user_id: UUID
  - entry_date: date
  - raw_text: string (1+ chars)
  - created_at: timestamp
  - updated_at: timestamp
  
  Kafka Topic: journal_entries
  Database: journal_entries table


        GEMINI AI EXTRACTION (2-3 seconds)
                    ↓


ParsedJournalEntry (Structured Output):
  - journal_entry_id: UUID (FK to JournalEntry)
  - user_id: UUID
  - entry_date: date
  - mood: {score, labels}
  - health: {sleep_hours, sleep_quality, energy_level, stress_level}
  - nutrition: {meals[], calories, macros}
  - exercise: {type, duration_minutes, intensity, distance}
  - work: {productivity_score, tasks[]}
  - habits: [{name, completed}]
  - confidence_score: float (0.0-1.0)
  - gaps_detected: string[]
  - parsed_at: timestamp
  
  Kafka Topic: parsed_entries
  Database: parsed_entries table


        EVENT EMISSION (Individual Topics)
                    ↓


Individual Events:

  MoodEntry:
    - user_id: UUID
    - timestamp: datetime
    - mood_score: int (1-10)
    - mood_labels: string[]
    - notes: string
    Kafka Topic: mood_events
    Table: mood_events (time-series)
  
  HealthMetrics:
    - user_id: UUID
    - timestamp: datetime
    - sleep_hours: float (0-24)
    - sleep_quality: int (1-10)
    - energy_level: int (1-10)
    - stress_level: int (1-10)
    - symptoms: string[]
    Kafka Topic: health_events
    Table: health_events (time-series)
  
  NutritionEntry:
    - user_id: UUID
    - timestamp: datetime
    - meal_type: string (breakfast, lunch, dinner, snack)
    - food_items: string[]
    - calories: int
    - protein_g: float
    - carbs_g: float
    - fat_g: float
    Kafka Topic: nutrition_events
    Table: nutrition_events (time-series)
  
  ExerciseEntry:
    - user_id: UUID
    - timestamp: datetime
    - exercise_type: string
    - duration_minutes: int
    - intensity: string (low, moderate, high)
    - calories_burned: int
    - distance_km: float
    Kafka Topic: exercise_events
    Table: exercise_events (time-series)
  
  WorkEvent:
    - user_id: UUID
    - timestamp: datetime
    - productivity_score: int (1-10)
    - focus_level: int (1-10)
    - tasks: string[]
    Kafka Topic: work_events
    Table: work_events (time-series)


        CONTEXT UPDATE
                    ↓


ContextUpdate (Aggregated State):
  - user_id: UUID
  - update_type: string (mood, health, nutrition, exercise, work, goal, task)
  - data: JSON (specific to update_type)
  - timestamp: datetime
  
  Kafka Topic: context_updates
  Subscriber: context-broker


6.3 USER ACCOUNT MODEL
───────────────────────

User:
  - id: UUID
  - email: EmailStr
  - username: string (3-100 chars)
  - full_name: string (nullable)
  - timezone: string (default: UTC)
  - created_at: timestamp
  - updated_at: timestamp
  - last_login: timestamp (nullable)
  - is_active: boolean
  - is_verified: boolean
  
  Database: users table
  Index: PRIMARY KEY (id)
  Index: UNIQUE (email)
  Index: UNIQUE (username)


6.4 KNOWLEDGE ITEM MODEL
──────────────────────────

KnowledgeItem:
  - id: UUID
  - user_id: UUID (FK)
  - title: string
  - content: text
  - tags: string[]
  - topics: string[]
  - source: string (journal, manual, import)
  - embedding: vector (768-dim) [stored in Qdrant]
  - created_at: timestamp
  - updated_at: timestamp
  - last_accessed: timestamp
  
  Database: knowledge_items table (PostgreSQL)
  Vector DB: Qdrant (embeddings)
  Index: user_id, created_at


================================================================================
7. AI INTEGRATION (GEMINI)
================================================================================

7.1 JOURNAL PARSER (Structured Extraction)
───────────────────────────────────────────

Model: gemini-2.5-flash (default)
Alternative: gemini-2.0-flash-exp

Features:
  - Structured schema (JSON schema validation)
  - Prompt engineering for multi-field extraction
  - Confidence scoring for each field
  - Caching enabled (reduces API costs ~25%)
  - Input validation before API call
  - Error handling and fallback

Request Format:
  {
    "prompt": "Extract structured data from this journal entry: ...",
    "schema": {
      "type": "object",
      "properties": {
        "mood_score": {"type": "number", "minimum": 1, "maximum": 10},
        "sleep_hours": {"type": "number", "minimum": 0, "maximum": 24},
        ...
      }
    }
  }

Response Format:
  {
    "mood_score": 8.0,
    "mood_labels": ["energized", "focused"],
    "sleep_hours": 8.0,
    "exercise_minutes": 30,
    ...
    "confidence_score": 0.95,
    "extraction_time_ms": 2340
  }

Cost Optimization:
  - Prompt caching: Cache repeated prompts
  - Batch processing: Process multiple entries
  - Token optimization: Structured output reduces tokens
  - Fallback: Cache for repeated extractions


7.2 KNOWLEDGE SYSTEM (Embeddings + Search)
──────────────────────────────────────────

Embedding Model: gemini-embedding-001

Features:
  - 768-dimensional vectors
  - Semantic similarity
  - Multi-language support
  - Fast inference (<100ms)

Extraction Model: gemini-2.0-flash-exp

Features:
  - Topic identification
  - Tag generation
  - Relationship discovery
  - Contextual enhancement

Workflow:
  1. User creates knowledge item
  2. Gemini extracts topics/tags
  3. Generate embedding vector
  4. Store in Qdrant with metadata
  5. Enable semantic search
  
Search Query:
  User Input: "productivity tips from my journal"
  → Generate embedding
  → Query Qdrant with similarity search
  → Return top-K similar items
  → Rank by relevance + recency


7.3 AI USAGE & BILLING
───────────────────────

Estimated Costs:
  - Journal parsing: $0.005 per entry (with caching)
  - Embeddings: $0.0001 per 1000 tokens
  - Knowledge extraction: $0.01 per item
  
Optimization:
  - Use caching for repeated operations
  - Batch operations when possible
  - Selective extraction (only changed fields)
  - Request deduplication


================================================================================
8. REQUEST FLOW EXAMPLES
================================================================================

8.1 CREATE JOURNAL ENTRY
─────────────────────────

Request:
  POST /api/journal
  Content-Type: application/json
  Authorization: Bearer <JWT_TOKEN>
  
  {
    "entry_date": "2025-01-15",
    "raw_text": "Woke up feeling great today..."
  }

Processing:

  1. API Gateway (Kong)
     - Extract JWT token
     - Validate signature
     - Extract user_id from claims
     - Rate limit check
     - Route to appropriate service
  
  2. Service Receives Request
     - Validate input schema
     - Check user permissions
     - Generate UUID for entry
  
  3. Store in Database
     PostgreSQL:
       INSERT INTO journal_entries 
       (id, user_id, entry_date, raw_text, created_at)
       VALUES (?, ?, ?, ?, NOW())
  
  4. Publish Kafka Event
     Topic: journal_entries
     Key: {user_id}
     Value: {
       "journal_entry_id": "...",
       "user_id": "...",
       "entry_date": "2025-01-15",
       "raw_text": "..."
     }
  
  5. Return Response
     HTTP 201 Created
     {
       "id": "...",
       "user_id": "...",
       "created_at": "2025-01-15T10:30:00Z"
     }

Background Processing (Async):

  6. Journal Parser Consumes
     - Kafka consumer receives message
     - Verifies format
     - Calls JournalParserOrchestrator

  7. Gemini Extraction (2-3 seconds)
     - Generate structured prompt
     - Call Gemini API
     - Parse response
     - Score confidence

  8. Publish Results
     - PARSED_ENTRIES event
     - Individual metric events (MOOD_EVENTS, etc.)
     - CONTEXT_UPDATES event

  9. Context Broker Processes
     - Subscribes to CONTEXT_UPDATES
     - Updates user_context table
     - Invalidates Redis cache

  10. Subsequent Context Request
      - Cache miss (invalidated)
      - Fresh query from DB
      - Returns updated state


8.2 GET USER CONTEXT
─────────────────────

Request:
  GET /api/context
  Authorization: Bearer <JWT_TOKEN>

Processing:

  1. API Gateway
     - Validate JWT
     - Extract user_id
     - Route to context-broker service

  2. Context Broker Receives
     - Log request
     - Begin latency timer

  3. Check Redis Cache
     Key: context:{user_id}
     
     IF key exists:
       value = Redis GET key
       IF value is valid:
         Increment cache_hit_counter
         Return value
         Latency: 5-10ms
       ELSE:
         Remove stale key
         Continue to DB query
     
     ELSE:
       Continue to DB query

  4. Query PostgreSQL (if cache miss)
     
     Query A: User Context
       SELECT * FROM user_context 
       WHERE user_id = ?
     
     Query B: 7-Day Mood Average
       SELECT AVG(mood_score) as avg_mood
       FROM mood_events
       WHERE user_id = ?
       AND created_at >= NOW() - INTERVAL 7 DAY
     
     Query C: 7-Day Productivity Average
       SELECT AVG(productivity_score) as avg_prod
       FROM work_events
       WHERE user_id = ?
       AND created_at >= NOW() - INTERVAL 7 DAY
     
     Query D: Goal Count
       SELECT COUNT(*) as active_goals
       FROM goals
       WHERE user_id = ? AND status = 'active'
     
     Query E: Task Count
       SELECT COUNT(*) as pending_tasks
       FROM tasks
       WHERE user_id = ? AND status != 'completed'
     
     Query F: Tasks Completed Today
       SELECT COUNT(*) as completed_today
       FROM tasks
       WHERE user_id = ? 
       AND completed_date = TODAY()
     
     Total Latency: 30-50ms

  5. Aggregate Data
     - Combine all query results
     - Create UserContext object
     - Add timestamps
     - Compute derived fields
     Latency: <1ms

  6. Cache Result
     Key: context:{user_id}
     Value: JSON serialized UserContext
     TTL: 300 seconds (5 minutes)
     Latency: 5ms

  7. Return Response
     HTTP 200 OK
     {
       "user_id": "...",
       "current_mood_score": 8,
       "current_energy_level": 6,
       "current_stress_level": 4,
       "sleep_quality_avg_7d": 7.2,
       "productivity_score_avg_7d": 6.8,
       "active_goals_count": 5,
       "pending_tasks_count": 12,
       "completed_tasks_today": 3,
       "updated_at": "2025-01-15T10:30:00Z"
     }
     
     Total Latency:
       - First request: 40-60ms (DB query)
       - Cached requests: 5-10ms (Redis)


================================================================================
9. CACHING STRATEGY
================================================================================

9.1 CACHE ARCHITECTURE
───────────────────────

Layer 1: HTTP/Browser Cache
  - Handled by frontend
  - Cache-Control headers from API
  - Max-age: 5 minutes for context

Layer 2: Redis Cache (Hot Data)
  - TTL: 5-30 minutes depending on data type
  - Location: In-memory, fast retrieval
  - Persistence: AOF enabled
  - Eviction: LRU (least recently used)

Layer 3: Database Cache
  - PostgreSQL buffer pool
  - Query result caching (implicit)
  - Connection pooling

Layer 4: Application Memory
  - Connection pools
  - Thread-local storage
  - Short-lived objects


9.2 CACHE KEY STRUCTURE
───────────────────────

Pattern: {namespace}:{user_id}:{variant}

Examples:
  - context:123e4567-e89b-12d3-a456-426614174000
    Cached UserContext object
    TTL: 5 minutes
  
  - mood:123e4567-e89b-12d3-a456-426614174000:2025-01-15
    Daily mood data
    TTL: 24 hours
  
  - health:123e4567-e89b-12d3-a456-426614174000:7d_avg
    7-day health average
    TTL: 1 hour
  
  - goals:123e4567-e89b-12d3-a456-426614174000:active
    Active goals list
    TTL: 10 minutes
  
  - tasks:123e4567-e89b-12d3-a456-426614174000:pending
    Pending tasks
    TTL: 5 minutes


9.3 CACHE INVALIDATION STRATEGY
─────────────────────────────────

Invalidation Triggers:

  On Mood Update:
    - DELETE context:{user_id}
    - DELETE mood:{user_id}:{date}
    - DELETE health:{user_id}:7d_avg (recalculate)

  On Task Update:
    - DELETE tasks:{user_id}:pending
    - DELETE context:{user_id} (task count changed)

  On Goal Update:
    - DELETE goals:{user_id}:active
    - DELETE context:{user_id}

  On Knowledge Item Creation:
    - No context invalidation needed
    - Update knowledge:{user_id} separately

TTL-Based Expiration:
  - Real-time data (mood, energy): 5 minutes
  - Aggregated data (7-day avg): 1 hour
  - Reference data (goals, tasks): 10 minutes
  - Session data: 30 minutes


9.4 CACHE HIT RATIO TARGET
───────────────────────────

Target: > 80% for context requests
Target: > 60% for task/goal requests

Optimization:
  - Pre-warm cache on service startup
  - Batch cache operations
  - Monitor hit/miss ratio in Prometheus
  - Adjust TTLs based on usage patterns


================================================================================
10. MONITORING & OBSERVABILITY
================================================================================

10.1 PROMETHEUS METRICS
────────────────────────

Service Metrics (All services):
  - request_duration_ms (histogram)
    Labels: service, endpoint, method, status_code
    Buckets: 10, 50, 100, 500, 1000, 5000
  
  - requests_total (counter)
    Labels: service, endpoint, method, status_code
  
  - active_connections (gauge)
    Labels: service
  
  - errors_total (counter)
    Labels: service, error_type
    Types: VALIDATION_ERROR, DB_ERROR, CACHE_ERROR, API_ERROR


Business Metrics (Domain-specific):
  - journal_entries_created_total (counter)
  - journal_entries_parsed_total (counter)
  - parsing_confidence_score (histogram)
  - ai_extraction_duration_ms (histogram)
  - context_updates_processed_total (counter)
  - knowledge_items_created_total (counter)
  - search_results_returned (counter)


Cache Metrics:
  - redis_get_hits_total (counter)
  - redis_get_misses_total (counter)
  - redis_memory_bytes (gauge)
  - redis_connected_clients (gauge)


Database Metrics:
  - db_query_duration_ms (histogram)
  - db_connections_active (gauge)
  - db_connections_total (counter)
  - db_errors_total (counter)


Kafka Metrics:
  - kafka_messages_consumed_total (counter)
    Labels: topic, consumer_group
  
  - kafka_consumer_lag (gauge)
    Labels: topic, partition, consumer_group
    Target: < 100 messages lag
  
  - kafka_produce_duration_ms (histogram)


10.2 GRAFANA DASHBOARDS
────────────────────────

Dashboard 1: System Health
  Panels:
    - Service availability (uptime %)
    - P50, P95, P99 latencies
    - Error rate (errors/requests %)
    - Active connections per service
    - Request volume (req/sec)

Dashboard 2: Business Metrics
  Panels:
    - Journal entries parsed (daily)
    - AI extraction confidence (avg)
    - Parse success rate (%)
    - Knowledge items created (cumulative)
    - User engagement (active users, entries/user)

Dashboard 3: Infrastructure
  Panels:
    - PostgreSQL CPU, memory, disk
    - Redis memory usage, evictions
    - Kafka broker health
    - Qdrant vector DB status
    - Disk space available

Dashboard 4: Kafka
  Panels:
    - Messages per topic
    - Consumer group lag
    - Partition distribution
    - Topic growth

Dashboard 5: Cache Performance
  Panels:
    - Cache hit ratio (%)
    - Redis operations/sec
    - Cache size (bytes)
    - Evictions per minute


10.3 ALERTING RULES
─────────────────────

Critical Alerts (PagerDuty):
  - Service down (uptime < 1 for 1 minute)
  - Error rate > 5%
  - P99 latency > 5000ms
  - Database connection pool exhausted
  - Kafka consumer lag > 10000 messages

Warning Alerts (Slack):
  - Error rate > 1%
  - P99 latency > 2000ms
  - Cache hit ratio < 60%
  - Redis memory > 80% full
  - Kafka consumer lag > 1000 messages


10.4 LOGGING STRATEGY
──────────────────────

Structured Logging (JSON format):
  All services output JSON logs to stdout
  Fields:
    - timestamp: ISO 8601
    - level: DEBUG, INFO, WARNING, ERROR
    - service: service name
    - request_id: UUID (correlation)
    - user_id: UUID (optional)
    - action: what happened
    - duration_ms: processing time
    - status: success/failure
    - error: error message (if applicable)

Log Levels:
  - DEBUG: Development, detailed flow
  - INFO: Key operations (entry parsed, context updated)
  - WARNING: Unusual conditions (cache miss, slow query)
  - ERROR: Failed operations (API call failed, DB error)

Log Retention:
  - Recent logs: 7 days hot storage
  - Archive: 90 days cold storage
  - Deletion: After 1 year


================================================================================
11. DESIGN PATTERNS
================================================================================

11.1 Architectural Patterns
─────────────────────────────

Event-Driven Architecture:
  - Services communicate via Kafka topics
  - Loose coupling between services
  - Asynchronous processing
  - Event sourcing for audit trail

Microservices:
  - Independent deployable units
  - Separate responsibility domains
  - Own data storage (except shared cache)
  - API contracts via OpenAPI/Swagger

API Gateway Pattern:
  - Single entry point (Kong)
  - Request routing and transformation
  - Authentication/authorization
  - Rate limiting and throttling

Cache-Aside Pattern:
  - Check cache first
  - On miss, fetch from DB
  - Populate cache for future requests
  - Used in Context Broker for sub-50ms latency


11.2 Data Patterns
────────────────────

Immutable Events:
  - Journal entries are immutable after creation
  - Edits create new events
  - Full audit trail maintained
  - Kafka retention: 7 days

Aggregation:
  - UserContext aggregates multiple data sources
  - 7-day rolling averages computed from events
  - Denormalized for read performance
  - Updated via Kafka events

Time-Series Data:
  - Health, mood, exercise metrics stored as events
  - TimescaleDB for efficient aggregation
  - Daily/weekly/monthly rollups
  - Retention: 2 years


11.3 Resilience Patterns
──────────────────────────

Circuit Breaker:
  - API calls to Gemini with exponential backoff
  - Fall back to cached responses
  - Fail gracefully on errors

Retry Logic:
  - Database queries: 3 retries with backoff
  - Kafka producer: automatic retry
  - API calls: configurable retry strategy

Health Checks:
  - All services expose /health endpoint
  - Docker container health monitoring
  - Kubernetes liveness/readiness probes
  - Service dependencies validated


11.4 Security Patterns
────────────────────────

Authentication:
  - JWT tokens (HS256 or RS256)
  - Token expiry: 1 hour
  - Refresh tokens: 30 days
  - Stored in secure HTTP-only cookies

Authorization:
  - Role-based access control (RBAC)
  - User can only access own data
  - Admin operations require admin role
  - API gateway validates permissions

Data Protection:
  - Passwords hashed with bcrypt
  - Sensitive fields encrypted in DB
  - TLS/HTTPS for all communications
  - API keys stored in environment variables (not code)


================================================================================
12. DEPLOYMENT & RUNTIME
================================================================================

12.1 DOCKER COMPOSE DEPLOYMENT
─────────────────────────────────

Container Orchestration:
  Tool: Docker Compose v3.9
  File: docker-compose.yml

Services (13 total):

  Infrastructure (5):
    1. postgres - PostgreSQL database
    2. redis - Redis cache
    3. zookeeper - Kafka coordination
    4. kafka - Message broker
    5. qdrant - Vector database

  API & Services (4):
    6. api-gateway - Kong
    7. context-broker - Context service
    8. journal-parser - Parser service
    9. knowledge-system - Knowledge service

  Monitoring (3):
    10. prometheus - Metrics collection
    11. grafana - Dashboards
    12. kafka-ui - Kafka UI

  Networking:
    - plos-network (bridge network)
    - Internal DNS: service names

Volumes (8):
    - postgres_data
    - redis_data
    - kafka_data
    - zookeeper_data
    - zookeeper_datalog
    - prometheus_data
    - grafana_data
    - qdrant_data


12.2 STARTUP SEQUENCE
──────────────────────

Order of Dependency:

  Phase 1 - Infrastructure (parallel):
    1. zookeeper (startup)
    2. postgres (startup)
    3. redis (startup)
    4. qdrant (startup)
    5. kafka (depends on zookeeper)
    
    Healthchecks: Each service validates connectivity
    Timeout: 30 seconds per service

  Phase 2 - API Gateway:
    6. api-gateway (depends on postgres healthy)
    
    Initialization: Kong DB migrations
    Healthcheck: kong health command
    Timeout: 30 seconds

  Phase 3 - Core Services:
    7. context-broker (depends on postgres, redis, kafka healthy)
    8. journal-parser (depends on postgres, kafka, context-broker healthy)
    9. knowledge-system (depends on postgres, kafka, qdrant healthy)
    
    Initialization: Connect to databases, start Kafka consumers
    Healthcheck: HTTP GET /health endpoint
    Timeout: 30 seconds

  Phase 4 - Monitoring:
    10. prometheus (startup)
    11. grafana (depends on prometheus)
    12. kafka-ui (depends on kafka)
    
    Initialization: Config loading, scrape setup
    Timeout: 30 seconds

Total Startup Time: ~2-3 minutes


12.3 ENVIRONMENT VARIABLES
────────────────────────────

Database Configuration:
  - POSTGRES_USER (default: postgres)
  - POSTGRES_PASSWORD (default: plos_db_secure_2025)
  - POSTGRES_DB (default: plos)
  - POSTGRES_PORT (default: 5432)

Cache Configuration:
  - REDIS_PASSWORD (default: plos_redis_secure_2025)
  - REDIS_PORT (default: 6379)

Kafka Configuration:
  - KAFKA_BOOTSTRAP_SERVERS (default: kafka:9092)
  - KAFKA_BROKERS (default: kafka:9092)

Vector DB Configuration:
  - QDRANT_URL (default: http://qdrant:6333)
  - QDRANT_API_KEY (default: qdrant_secure_key_2025)

API Configuration:
  - GEMINI_API_KEY (required - no default)
  - GEMINI_DEFAULT_MODEL (default: gemini-2.5-flash)
  - GEMINI_EMBEDDING_MODEL (default: gemini-embedding-001)
  - USE_GEMINI_CACHING (default: true)

Service Configuration:
  - API_GATEWAY_PORT (default: 8000)
  - CONTEXT_BROKER_PORT (default: 8001)
  - JOURNAL_PARSER_PORT (default: 8002)
  - KNOWLEDGE_SYSTEM_PORT (default: 8003)
  - LOG_LEVEL (default: INFO)

Monitoring Configuration:
  - PROMETHEUS_PORT (default: 9090)
  - GRAFANA_PORT (default: 3333)
  - KAFKA_UI_PORT (default: 8080)


12.4 HEALTH CHECKS
────────────────────

PostgreSQL:
  Command: pg_isready -U postgres
  Interval: 10s
  Timeout: 5s
  Retries: 5
  Success Criteria: Accepts connections

Redis:
  Command: redis-cli INCR ping
  Interval: 10s
  Timeout: 5s
  Retries: 5
  Success Criteria: Returns integer response

Kafka:
  Command: kafka-broker-api-versions --bootstrap-server localhost:9092
  Interval: 10s
  Timeout: 10s
  Retries: 5
  Success Criteria: Lists broker versions

Services:
  Endpoint: GET /health
  Interval: 10s
  Timeout: 5s
  Retries: 5
  Success Criteria: HTTP 200 OK


12.5 RESTART POLICIES
───────────────────────

All services: restart: unless-stopped
  - Automatically restart if container crashes
  - Stay down if manually stopped
  - Restart after system reboot


================================================================================
13. KEY DESIGN PRINCIPLES
================================================================================

13.1 Performance
──────────────────

Target Latencies:
  - GET /context: < 50ms (cached), < 100ms (uncached)
  - Journal parsing: 2-3 seconds (AI extraction)
  - Knowledge search: < 200ms
  - API responses: < 500ms (p95)

Optimization Strategies:
  - Redis caching for hot data
  - Query optimization with proper indexes
  - Partitioned Kafka topics for parallelism
  - Connection pooling for databases
  - Batch operations where possible


13.2 Scalability
─────────────────

Horizontal Scaling:
  - Stateless service instances (can add more)
  - Load balancing via API Gateway
  - Distributed Kafka consumer groups
  - Database connection pooling
  - Redis cluster support (not currently used)

Vertical Scaling:
  - PostgreSQL hardware upgrades
  - Increased Kafka partitions
  - Redis memory increase
  - Service CPU/memory allocation


13.3 Reliability
──────────────────

High Availability:
  - Health checks for automatic recovery
  - Graceful degradation on failures
  - Circuit breakers for external APIs
  - Message durability via Kafka
  - Database backups (to be configured)

Error Handling:
  - Validation at API Gateway
  - Try-catch in all service operations
  - Structured error responses
  - Detailed logging for debugging


13.4 Security
───────────────

Data Protection:
  - Encrypted data in transit (HTTPS/TLS)
  - Encrypted at rest (PostgreSQL encryption)
  - API key rotation strategy
  - No sensitive data in logs

Access Control:
  - JWT authentication
  - RBAC authorization
  - API Gateway enforcement
  - User isolation (own data only)


13.5 Observability
─────────────────────

Monitoring:
  - Prometheus metrics from all services
  - Grafana dashboards for visualization
  - Kafka consumer lag tracking
  - Custom business metrics

Logging:
  - Structured JSON logs
  - Correlation IDs for tracing
  - Appropriate log levels
  - Retention policy

Tracing:
  - OpenTelemetry ready (future)
  - Request ID propagation
  - Service-to-service tracing


13.6 Maintainability
──────────────────────

Code Organization:
  - Clear separation of concerns
  - Reusable shared models
  - Consistent patterns
  - Type hints throughout

Documentation:
  - OpenAPI/Swagger specs
  - README files
  - Architecture docs (this file)
  - Code comments for complex logic

Testing:
  - Unit tests for business logic
  - Integration tests for data flow
  - CI/CD pipeline (GitHub Actions)
  - Linting and formatting (Black, isort)


================================================================================
QUICK REFERENCE
================================================================================

Service Ports:
  - API Gateway: http://localhost:8000
  - Context Broker: http://localhost:8001
  - Journal Parser: http://localhost:8002
  - Knowledge System: http://localhost:8003
  - PostgreSQL: localhost:5432
  - Redis: localhost:6379
  - Kafka: localhost:9092
  - Qdrant: http://localhost:6333
  - Kafka UI: http://localhost:8080
  - Prometheus: http://localhost:9090
  - Grafana: http://localhost:3333

Key Commands:
  Start all: ./scripts/start-all.ps1 (Windows) or ./scripts/dev.sh (Linux)
  View logs: docker-compose logs -f
  Health check: docker-compose ps
  Stop: docker-compose down
  Clean: ./scripts/clean.sh

Health Endpoints:
  Context Broker: http://localhost:8001/health
  Journal Parser: http://localhost:8002/health
  Knowledge System: http://localhost:8003/health

Kafka Topics (useful for debugging):
  docker exec -it plos-kafka kafka-console-consumer \
    --bootstrap-server localhost:9092 \
    --topic journal_entries \
    --from-beginning

Redis CLI:
  docker exec -it plos-redis redis-cli
  > KEYS *
  > GET context:*
  > MONITOR (watch all operations)

PostgreSQL CLI:
  docker exec -it plos-postgres psql -U postgres -d plos
  > SELECT * FROM user_context;
  > SELECT COUNT(*) FROM journal_entries;

Qdrant Dashboard:
  http://localhost:6333/dashboard


================================================================================
GLOSSARY
================================================================================

CQRS: Command Query Responsibility Segregation
  - Separate models for read and write operations

Event Sourcing:
  - Store state changes as immutable events
  - Replay events to reconstruct state

Kafka Consumer Group:
  - Multiple consumers share work on topics
  - Each partition assigned to one consumer

TimescaleDB:
  - PostgreSQL extension for time-series data
  - Automatic partitioning and compression

Vector Embedding:
  - Numerical representation of text/concepts
  - Enables semantic similarity search

SSOT: Single Source of Truth
  - One authoritative data store (UserContext)

TTL: Time To Live
  - How long data persists before expiration

JWT: JSON Web Token
  - Stateless authentication token

RBAC: Role-Based Access Control
  - Permissions based on user roles

OLTP: Online Transaction Processing
  - Database optimized for frequent updates

AOF: Append-Only File
  - Redis persistence mechanism

LRU: Least Recently Used
  - Cache eviction policy

P50/P95/P99: Percentiles
  - 50th/95th/99th percentile latencies


================================================================================
END OF ARCHITECTURE DOCUMENT
================================================================================

For the latest code and implementation details, refer to:
- Repository: https://github.com/Sathish111j/LifeOSbackend
- Service READMEs: ./services/*/README.md
- API Documentation: http://localhost:8000/docs (when running)
